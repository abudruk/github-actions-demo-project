name: Display Changed Files 
on:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main

jobs:
  archive-collections:
    name: Archive Collections
    strategy:
      matrix:
        type:
          - blueprints
          - ui-extension-packages
          - cb-applets
          - orchestration-actions
          - recurring-jobs
          - server-actions
          - resource-actions

    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v40
        with:
          files: |
             **.zip

      # - name: List all changed files
      #   run: |

      #     # Replace ".zip" with ";" in entire string
      #     changed_files_with_semicolon=$(echo "${{ steps.changed-files.outputs.all_changed_files }}" | sed 's/.zip/;/g')

      #     echo "files_with_semicolon $changed_files_with_semicolon"

      #     # split the string based on ';'  to  get proper array 
      #     IFS=';' read -ra changed_files_array <<< "$changed_files_with_semicolon"

      #     echo "changed_files_array :  ${changed_files_array[@]} "

      #     for file in "${changed_files_array[@]}"; do
      #       echo "${file}.zip was changed"
      #     done

          
      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: Create ${{ matrix.type }} Artifacts
        working-directory: CMP/${{ matrix.type }}
        run: |
          shopt -s globstar
          mkdir -p "${{github.workspace}}/tmp/${{ matrix.type }}"

          echo "pwd : "
          pwd
          # Replace ".zip" with ";" in entire string
          changed_files_with_semicolon=$(echo "${{ steps.changed-files.outputs.all_changed_files }}" | sed 's/.zip/;/g')

          echo "files_with_semicolon $changed_files_with_semicolon"

          # split the string based on ';'  to  get proper array 
          IFS=';' read -ra changed_files_array <<< "$changed_files_with_semicolon"

          echo "changed_files_array :  ${changed_files_array[@]} "


          # Initialize an empty array to store the extracted values
          changed_files_new_array=()

          # Loop through each element, extract the last part, and add it to the array
          for element in "${changed_files_array[@]}"; do
              # Use awk to extract the last part after the last '/'
              value=$(echo "$element" | awk -F'/' '{print $NF}')
              changed_files_new_array+=("${value}/${value}.zip")
          done

          

          echo "New changed files array : $changed_files_new_array"

          

          for file in "${changed_files_new_array[@]}"; do
            echo "Moving ${file}"

            BASENAME=$(basename "${file}")

            echo "BASENAME : $BASENAME"
            if [ -e "${file}" ]; then
              #apply xargs to remove any trailing whitespaces in string 
              mv "$(echo "$file" | xargs)" "${{ github.workspace }}/tmp/${{ matrix.type }}/$BASENAME"
            else
              echo "Warning: Source file does not exist. Move operation skipped."
            fi
          done

      - name: Create a single tar of all the files in the current directory
        working-directory: tmp/${{ matrix.type }}
        env:
          MATRIX_TYPE: ${{ matrix.type }}
        run: |
          echo "pwd : "
          pwd 

          WORKING_DIRECTORY="${{ github.workspace }}/tmp/${{ matrix.type }}" 

          echo "WORKING_DIRECTORY : ${WORKING_DIRECTORY}"
           

          echo "File is this director"
          ls

          echo "Check if Directory Exists"
          if [ -d "$WORKING_DIRECTORY" ]; then
            echo "Creating a tar of all the files in the current directory"
            tar -cvzf "$MATRIX_TYPE".tar.gz *
          else
            echo "Directory does not exist."
          fi

      - uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}
          path: |
            **/${{ matrix.type }}.tar.gz


  generate-assets:
    name: Generate Assets
    needs:
      - archive-collections
    runs-on: ubuntu-latest
    strategy:
      matrix:
        type:
          - blueprints
          - ui-extension-packages
          - cb-applets
          - orchestration-actions
          - recurring-jobs
          - server-actions
          - resource-actions
    steps:
      - name: Cleanup Files
        run: |
          rm -rf ./*

      - name: Download All Artifacts
        uses: actions/download-artifact@v3
        with:
          name: ${{ matrix.type }}
          path: downloads/${{matrix.type}}

      - name: print directory tree
        run: |
          sudo apt-get install tree -y
          echo "Current directory"
          pwd
          echo "Tree from pwd"
          tree

      - name: Untar all artifacts in tmp directory
        run: |
          rm -f *.zip
          echo "current dir:"
          pwd
          cd downloads/${{matrix.type}}/tmp/
          echo "Untarring ${{ matrix.type }}.tar.gz"
          tar -xvzf "${{ matrix.type }}.tar.gz"
          cd ../../../

      - name: print directory tree
        run: |
          sudo apt-get install tree -y
          tree

      - name: Generate SHA file for every zip file
        shell: bash -l {0}
        run: |
          for f in downloads/${{matrix.type}}/tmp/*.zip; do
            echo "Generating SHA for $f"
            sha256sum "$f" >> sha256sum.txt
          done
          cat ./sha256sum.txt
          cd ../../../

      - name: Move all .zip files to ./zip/ and rename with SHA and add path to metadata.json
        shell: bash -l {0}
        env:
          minimum_version: "9.4.6.1"
        run: |
          set -x
          # WARNING: This script will skip collections with special characters in their name.
          function set_variable {
            local varname="$1"
            local value="$2"
            local default="$3"
            # if value is empty, set it to default value
            if [ -z "$value" ]; then
              value="$default"
            fi
            printf -v "$varname" '%s' "$value"
            echo "Setting $varname to $value"
          }

          function find_sha_in_file {
            local SHAFILE="$1"
            local BASENAME="$2"
            local SHA_VAR="$3"
            set_variable "$SHA_VAR" "$(grep <"$SHAFILE" "$BASENAME" | tail -n 1 | awk '{print $1}')"
          }

          set_minimum_version_in_metadata() {
            local DEFAULT_VERSION="${{env.minimum_version}}"
            local MINIMUM_VERSION_REQUIRED=$(jq -r 'try(.minimum_version_required) // "empty"' "$METADATA_PATH")

            echo "DEFAULT_VERSION: $DEFAULT_VERSION"
            echo "MINIMUM_VERSION_REQUIRED: $MINIMUM_VERSION_REQUIRED"
            cat "$METADATA_PATH"

            if [[ "$MINIMUM_VERSION_REQUIRED" == "empty" ]]; then
              jq --arg version "$DEFAULT_VERSION" '.minimum_version_required = $version' "$METADATA_PATH" > "$METADATA_PATH.tmp"
              mv "$METADATA_PATH.tmp" "$METADATA_PATH"
            fi

          }

          add_artifact_path_to_metadata() {
            local METADATA_PATH="$1"
            local ARTIFACT_PATH="$2"
            local SHA="$3"
            cat "$METADATA_PATH" | jq .
            cat "$METADATA_PATH" | jq --arg artifactpath "$ARTIFACT_PATH" \
              --arg sha "$SHA" '. + {"artifact_path": $artifactpath } + {"sha": $sha }' >"$METADATA_PATH.tmp"
            mv "$METADATA_PATH.tmp" "$METADATA_PATH"
          }

          add_image_path_to_metadata() {
            local METADATA_PATH="$1"
            local IMAGE_PATH="$2"
            echo "$IMAGE_PATH"
            cat "$METADATA_PATH" | jq .
            cat "$METADATA_PATH" | jq --arg imagepath "$IMAGE_PATH" '. + {"library-image-url": $imagepath }' >"$METADATA_PATH.tmp"
            mv "$METADATA_PATH.tmp" "$METADATA_PATH"
          }

          add_collection_type_to_metadata() {
            local METADATA_PATH="$1"
            local COLLECTION_TYPE="$2"
            jq . "$METADATA_PATH"
            cat "$METADATA_PATH" | jq --arg collectiontype "$COLLECTION_TYPE" '. + {"collection_type": $collectiontype }' >"$METADATA_PATH.tmp"
            mv "$METADATA_PATH.tmp" "./$METADATA_PATH"
          }

          log_json() {
            local JSON="$1"
            echo "$JSON" | jq -r '.'
          }

          move_and_log_file() {
            local SRC="$1"
            local DST="$2"
            mkdir -p "$(dirname "$DST")"
            mv "$SRC" "$DST"
            echo "Moved $SRC to $DST"
          }

          extract_icon_from_zip() {
            local ZIP="$1"
            local METADATA_PATH="$2"
            cat "$METADATA_PATH"
            local ICON=$(jq -r ".icon" "$METADATA_PATH")
            mkdir -p "./tmp_images"
            echo "ICON: $ICON"
            unzip -j "$ZIP" -d "./tmp_images"
            ls ./tmp_images
            local ICON_SHA=$(sha256sum "./tmp_images/$ICON" | awk '{print $1}')
            local EXTENSION="${ICON##*.}"
            mkdir -p "./images"
            set_variable "ICON_FILENAME" "$ICON_SHA.$EXTENSION"
            echo "ICON_FILENAME: $ICON_FILENAME"
            mv ./tmp_images/"$ICON" ./images/"$ICON_FILENAME"
            rm -rf ./tmp_images
          }

          extract_metadata_from_zip() {
            local ZIP="$1"
            local DST="$2"
            local COLLECTION_NAME="$3"
            mkdir -p "$(dirname "$DST")"
            unzip -p "$ZIP" "*.json" >"$DST"
            echo "Extracted Metadata from $ZIP to $DST"
          }

          for f in downloads/${{matrix.type}}/tmp/*.zip; do
            echo "Processing $f ..."
            set_variable "ZIPFILE" "$f"
            set_variable "BASENAME" "$(basename "$ZIPFILE")"
            find_sha_in_file "sha256sum.txt" "$BASENAME" "SHA"
            set_variable "COLLECTION_PATH" "${ZIPFILE%.*}"
            set_variable "COLLECTION_NAME" "${BASENAME%.*}"
            extract_metadata_from_zip "$ZIPFILE" "metadata/$COLLECTION_NAME Metadata.json" "$COLLECTION_NAME"
            set_variable "METADATA_PATH" "metadata/$COLLECTION_NAME Metadata.json"
            set_variable "METADATA_FILENAME" "$(basename "$METADATA_PATH")"
            extract_icon_from_zip "$ZIPFILE" "$METADATA_PATH"
            set_variable "ARTIFACT_PATH" "zip/$SHA.zip"
            add_artifact_path_to_metadata "$METADATA_PATH" "$ARTIFACT_PATH" "$SHA"
            add_image_path_to_metadata "$METADATA_PATH" "images/$ICON_FILENAME"
            log_json "$METADATA_PATH"
            add_collection_type_to_metadata "$METADATA_PATH" "${{matrix.type}}"
            set_minimum_version_in_metadata
            move_and_log_file "$ZIPFILE" "./zip/${SHA}.zip"
            move_and_log_file "$METADATA_PATH" "./asset_files/${METADATA_FILENAME}"
          done

      - name: Remove tmp directory
        run: |
          rm -vrf tmp

      - name: Remove CMP directory
        run: |
          rm -vrf CMP

      - name: Remove metadata directory
        run: |
          rm -vrf metadata

      - name: Remove OneFuse directory
        run: |
          rm -vrf OneFuse

      - name: Remove downloads directory
        run: |
          rm -vrf downloads

      - name: Remove sha256sum.txt
        run: |
          rm -vrf sha256sum.txt
          rm -vrf sha256sum_png.txt

      - name: Create ${{ matrix.type }}.json
        env:
          MATRIX_TYPE: "${{ matrix.type }}"
        run: |
          for f in ./*/*.json; do
            echo "Processing $f ..."
            DIRNAME="$(dirname "$f")"
            # awk print last directory
            LAST_DIRNAME="$(awk -F/ '{print $NF}' <<<"$DIRNAME")"
            jq -n '[inputs]' ./$LAST_DIRNAME/*.json > "$LAST_DIRNAME/$MATRIX_TYPE".json
          done

      # - name: Print and Format JSON 
      #   run: |
      #     echo "current dir:"
      #     pwd
      #     cat "./asset_files/${{ matrix.type }}.json" | jq

      # Download blueprint.json file from bucket and update and reupload again 

      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: List folders in S3 bucket
        id: list-folders
        run: |
          folders=$(aws s3api list-objects-v2 --bucket abbybucketcloudformation-dont-delete --delimiter "/" --query "CommonPrefixes[].Prefix" --output text)
          echo "::set-output name=folders::${folders}"


      - name: Download , Update and Upload JSON objects
        run: |

          if [[ "${{ matrix.type }}" == "blueprints" || "${{ matrix.type }}" == "recurring-jobs" || "${{ matrix.type }}" == "orchestration-actions" ]]; then
            comparing_key='name' 
          else
            comparing_key='label'
          fi

          echo "Comparing key is : ${comparing_key}"

          echo "Folders ${{ steps.list-folders.outputs.folders }}"
          IFS=$'\t' read -ra folders <<< "${{ steps.list-folders.outputs.folders }}"
          for folder in "${folders[@]}"; do
            folder="${folder%/}"  # Remove trailing slash
            echo "Processing folder: $folder"


            # Download JSON file from the folder
            echo "Downloading  File : ${{ matrix.type }}.json"

            # Check if File Exists 
            echo "Check if File Exists at $folder/${{ matrix.type }}.json"
            if aws s3api head-object --bucket "abbybucketcloudformation-dont-delete" --key "$folder/${{ matrix.type }}.json" 2>&1 | grep -q "Not Found"; then
              echo "File not found in S3. Skipping download."
            else
              # Download the file from Bitbucket
              echo " Download the file $folder/${{ matrix.type }}.json" 
              aws s3 cp "s3://abbybucketcloudformation-dont-delete/$folder/${{ matrix.type }}.json" .
              echo "File downloaded successfully."

              echo "Quering and Updating File where name is ADO Pipeline" 

              jq -c '.[]' "./asset_files/${{ matrix.type }}.json" | while read -r object; do
                # echo "Object : $object"
                name=$(echo "$object" | jq -r '.name')
                artifact_path=$(echo "$object" | jq -r '.artifact_path')
                sha=$(echo "$object" | jq -r '.sha')
                libraryimageurl=$(echo "$object" | jq -r '.["library-image-url"]')
                
                echo "name : $name"
                echo "artifact_path : $artifact_path"
                echo "sha : $sha"
                echo "library-image-url : $libraryimageurl}"


                jq "map(if .\"$comparing_key\" == \"$name\" then .artifact_path = \"$artifact_path\" | .sha = \"$sha\" | .[\"library-image-url\"] = \"$libraryimageurl\" else . end)" ${{ matrix.type }}.json > modified_${{ matrix.type }}_new.json

                mv "modified_${{ matrix.type }}_new.json" "${{ matrix.type }}.json"
              done
              
              echo "Uploading File ${{ matrix.type }}.json to folder ${folder}"   
              aws s3 mv ${{ matrix.type }}.json "s3://abbybucketcloudformation-dont-delete/$folder/${{ matrix.type }}.json"
            fi
          done


      - name: display directory tree
        run: |
          sudo apt-get install tree -y
          tree -L 5
          ls

      - name: Remove '*Metadata.json' files
        run: |
          shopt -s globstar
          rm -vf **/*Metadata.json

      - name: display directory tree
        run: |
          sudo apt-get install tree -y
          tree -L 5

      - name: Create a single tar of all the files in the current directory
        run: |
          echo "Creating a tar.gz of all the files in the current directory"
          tar --exclude-vcs --ignore-failed-read -czvf assets.tar.gz *

      - uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }} Assets
          path: |
            **/*.tar.gz

      # - uses: act10ns/slack@v2
      #   if: failure()
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_DEVOPS_GITHUB_ACTION_ALERTS_WEBHOOK }}
      #   with:
      #     status: ${{ job.status }}
      #     steps: ${{ toJson(steps) }}
  deploy-to-staging:
    name: Deploy to Staging
    needs:
      - generate-assets
    # if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'Staging')
    runs-on: ubuntu-latest
    steps:
      - name: Set up AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: Deploy to Staging
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          gh workflow run publish-artifact.yml \
          --field="run_id=${{ github.run_id }}" \
          --field="environment=Staging" \
          --ref=main \
          --repo abudruk/github-actions-demo-project
     
  # changed_files:
  #   name: Test changed-files
  #   strategy:
  #     matrix:
  #       type:
  #         - blueprints
  #         # - ui-extension-packages
  #         # - cb-applets
  #         # - orchestration-actions
  #         # - recurring-jobs
  #         # - server-actions
  #         # - resource-actions

  #   runs-on: ubuntu-latest  
    
  #   steps:
  #     - uses: actions/checkout@v4
  #       with:
  #         fetch-depth: 0  # OR "2" -> To retrieve the preceding commit.

          
      # - name: Set up AWS CLI
      #   uses: aws-actions/configure-aws-credentials@v2
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ap-south-1
      
  #     - name: List folders in S3 bucket
  #       id: list-folders
  #       run: |
  #         folders=$(aws s3api list-objects-v2 --bucket abbybucketcloudformation-dont-delete --delimiter "/" --query "CommonPrefixes[].Prefix" --output text)
  #         echo "::set-output name=folders::${folders}"

  #     - name: Download and Update and Upload JSON objects
  #       run: |
  #         echo "Folders ${{ steps.list-folders.outputs.folders }}"
  #         IFS=$'\t' read -ra folders <<< "${{ steps.list-folders.outputs.folders }}"
  #         for folder in "${folders[@]}"; do
  #           folder="${folder%/}"  # Remove trailing slash
  #           echo "Processing folder: $folder"


  #           # Download JSON file from the folder
  #           echo "Downloading  File : ${{ matrix.type }}.json"

  #           # Check if File Exists 
  #           echo "Check if File Exists at $folder/${{ matrix.type }}.json"
  #           if aws s3api head-object --bucket "abbybucketcloudformation-dont-delete" --key "$folder/${{ matrix.type }}.json" 2>&1 | grep -q "Not Found"; then
  #             echo "File not found in S3. Skipping download."
  #           else
  #             # Download the file 
  #             echo " Download the file $folder/${{ matrix.type }}.json" 
  #             aws s3 cp "s3://abbybucketcloudformation-dont-delete/$folder/${{ matrix.type }}.json" .
  #             aws s3 cp "s3://abbybucketcloudformation-dont-delete/${{ matrix.type }}.json" "./local_{{ matrix.type }}_json.json"
  #             echo "File downloaded successfully."

  #             echo "Quering and Updating File where name is ADO Pipeline" 

  #             jq -c '.[]' "local_{{ matrix.type }}_json.json" | while read -r object; do
  #               echo "Object : $object"
  #               name=$(echo "$object" | jq -r '.name')
  #               artifact_path=$(echo "$object" | jq -r '.artifact_path')
  #               sha=$(echo "$object" | jq -r '.sha')
  #               libraryimageurl=$(echo "$object" | jq -r '.["library-image-url"]')
                
                

  #               echo "name : $name"
  #               echo "artifact_path : $artifact_path"
  #               echo "sha : $sha"
  #               echo "library-image-url : $libraryimageurl}"

  #               jq "map(if .name == \"$name\" then .artifact_path = \"$artifact_path\" | .sha = \"$sha\" | .[\"library-image-url\"] = \"$libraryimageurl\" else . end)" ${{ matrix.type }}.json > modified_${{ matrix.type }}_new.json

  #               # jq 'map(if .name == "$name" then .artifact_path = "$artifact_path" | .sha = "$sha" | .["library-image-url"] = "$libraryimageurl" else . end)' ${{ matrix.type }}.json > modified_${{ matrix.type }}_new.json

  #               mv "modified_${{ matrix.type }}_new.json" "${{ matrix.type }}.json"
  #             done
              

  #             # echo "Uploading File ${{ matrix.type }}.json to folder ${folder}"   
  #             # aws s3 cp ${{ matrix.type }}.json "s3://abbybucketcloudformation-dont-delete/$folder/${{ matrix.type }}.json"
  #           fi
  #         done

  #     - name: Print and Format JSON 
  #       run: |
  #         cat "${{ matrix.type }}.json"  | jq
     


          

            
